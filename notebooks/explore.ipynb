{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import math\n",
    "\n",
    "from typing import Optional, Tuple, Type\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        mlp_dim: int,\n",
    "        act: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n",
    "        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.lin2(self.act(self.lin1(x)))\n",
    "\n",
    "\n",
    "# From https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py # noqa\n",
    "# Itself from https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119  # noqa\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, D_features, mlp_ratio=0.25, act_layer=nn.GELU, skip_connect=True):\n",
    "        super().__init__()\n",
    "        self.skip_connect = skip_connect\n",
    "        D_hidden_features = int(D_features * mlp_ratio)\n",
    "        self.act = act_layer()\n",
    "        self.D_fc1 = nn.Linear(D_features, D_hidden_features)\n",
    "        self.D_fc2 = nn.Linear(D_hidden_features, D_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (BT, HW+1, D)\n",
    "        xs = self.D_fc1(x)\n",
    "        xs = self.act(xs)\n",
    "        xs = self.D_fc2(xs)\n",
    "        if self.skip_connect:\n",
    "            x = x + xs\n",
    "        else:\n",
    "            x = xs\n",
    "        return x\n",
    "    \n",
    "# This class and its supporting functions below lightly adapted from the ViTDet backbone available at: https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py # noqa\n",
    "class ImageEncoderViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        args,\n",
    "        img_size: int = 1024,\n",
    "        patch_size: int = 16,\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        depth: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        out_chans: int = 256,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        use_abs_pos: bool = True,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        window_size: int = 0,\n",
    "        global_attn_indexes: Tuple[int, ...] = (),\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Input image size.\n",
    "            patch_size (int): Patch size.\n",
    "            in_chans (int): Number of input image channels.\n",
    "            embed_dim (int): Patch embedding dimension.\n",
    "            depth (int): Depth of ViT.\n",
    "            num_heads (int): Number of attention heads in each ViT block.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            norm_layer (nn.Module): Normalization layer.\n",
    "            act_layer (nn.Module): Activation layer.\n",
    "            use_abs_pos (bool): If True, use absolute positional embeddings.\n",
    "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            window_size (int): Window size for window attention blocks.\n",
    "            global_attn_indexes (list): Indexes for blocks using global attention.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.in_chans = in_chans\n",
    "        self.args = args\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            kernel_size=(patch_size, patch_size),\n",
    "            stride=(patch_size, patch_size),\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "\n",
    "        self.pos_embed: Optional[nn.Parameter] = None\n",
    "        if use_abs_pos:\n",
    "            # Initialize absolute positional embedding with pretrain image size.\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim)\n",
    "            )\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            block = Block(\n",
    "                args= self.args,\n",
    "                block_number=i,\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "                use_rel_pos=use_rel_pos,\n",
    "                rel_pos_zero_init=rel_pos_zero_init,\n",
    "                window_size=window_size if i not in global_attn_indexes else 0,\n",
    "                input_size=(img_size // patch_size, img_size // patch_size),\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embed_dim,\n",
    "                out_chans,\n",
    "                kernel_size=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            LayerNorm2d(out_chans),\n",
    "            nn.Conv2d(\n",
    "                out_chans,\n",
    "                out_chans,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            LayerNorm2d(out_chans),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.patch_embed(x)\n",
    "        if self.pos_embed is not None:\n",
    "            x = x + self.pos_embed\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.neck(x.permute(0, 3, 1, 2))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer blocks with support of window attention and residual propagation blocks\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        args,\n",
    "        block_number: int,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        scale: float = 0.5,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        window_size: int = 0,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads in each ViT block.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            norm_layer (nn.Module): Normalization layer.\n",
    "            act_layer (nn.Module): Activation layer.\n",
    "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            window_size (int): Window size for window attention blocks. If it equals 0, then\n",
    "                use global attention.\n",
    "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
    "                positional parameter size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            use_rel_pos=use_rel_pos,\n",
    "            rel_pos_zero_init=rel_pos_zero_init,\n",
    "            input_size=input_size if window_size == 0 else (window_size, window_size),\n",
    "        )\n",
    "\n",
    "        self.block_number = block_number\n",
    "\n",
    "        # assert\n",
    "\n",
    "        if self.args.image_encoder_configuration[block_number] == 3:  # space adapter + MLP adapter\n",
    "            self.MLP_Adapter = Adapter(dim, skip_connect=False)  # MLP-adapter, no skip connection\n",
    "            self.Space_Adapter = Adapter(dim)  # with skip connection\n",
    "            self.scale = scale\n",
    "\n",
    "        elif self.args.image_encoder_configuration[block_number] == 2:  # only MLP adapter.\n",
    "            self.MLP_Adapter = Adapter(dim, skip_connect=False)\n",
    "            self.scale = scale\n",
    "\n",
    "        elif self.args.image_encoder_configuration[block_number] == 1:  # only space adapter\n",
    "            self.Space_Adapter = Adapter(dim)  # with skip connection\n",
    "\n",
    "        elif self.args.image_encoder_configuration[block_number] == 0:  # original sam\n",
    "            pass\n",
    "\n",
    "        # self.MLP_Adapter = Adapter(dim, skip_connect=False)  # MLP-adapter, no skip connection\n",
    "        # self.Space_Adapter = Adapter(dim)  # with skip connection\n",
    "        # self.scale = scale\n",
    "        # self.Depth_Adapter = Adapter(dim, skip_connect=False)  # no skip connection\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer)\n",
    "\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shortcut = x\n",
    "        # Window partition\n",
    "        if self.window_size > 0:\n",
    "            H, W = x.shape[1], x.shape[2]\n",
    "            x, pad_hw = window_partition(x, self.window_size)\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "\n",
    "        if self.args.image_encoder_configuration[self.block_number] in [1, 3]:   # [only space adapter, space adapter + MLP adapter]\n",
    "            x = self.Space_Adapter(x)\n",
    "\n",
    "        # if self.args.thd:\n",
    "        #     xd = rearrange(xd, 'b (hh ww) c -> b  hh ww c', hh= hh )\n",
    "        #     x = x + xd\n",
    "\n",
    "        # Reverse window partition\n",
    "        if self.window_size > 0:\n",
    "            x = window_unpartition(x, self.window_size, pad_hw, (H, W))\n",
    "\n",
    "        x = shortcut + x\n",
    "\n",
    "        if self.args.image_encoder_configuration[self.block_number] in [0, 1]:  # [original sam, only space adapter]\n",
    "            x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        if self.args.image_encoder_configuration[self.block_number] in [2, 3]:  # [only MLP adapter, space adapter + MLP adapter]\n",
    "            xn = self.norm2(x)\n",
    "            x = x + self.mlp(xn) + self.scale * self.MLP_Adapter(xn)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = True,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n",
    "            rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
    "                positional parameter size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.use_rel_pos = use_rel_pos\n",
    "        if self.use_rel_pos:\n",
    "            assert (\n",
    "                input_size is not None\n",
    "            ), \"Input size must be provided if using relative positional encoding.\"\n",
    "            # initialize relative positional embeddings\n",
    "            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n",
    "            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W, _ = x.shape\n",
    "        # qkv with shape (3, B, nHead, H * W, C)\n",
    "        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        # q, k, v with shape (B * nHead, H * W, C)\n",
    "        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\n",
    "\n",
    "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
    "\n",
    "        if self.use_rel_pos:\n",
    "            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Partition into non-overlapping windows with padding if needed.\n",
    "    Args:\n",
    "        x (tensor): input tokens with [B, H, W, C].\n",
    "        window_size (int): window size.\n",
    "\n",
    "    Returns:\n",
    "        windows: windows after partition with [B * num_windows, window_size, window_size, C].\n",
    "        (Hp, Wp): padded height and width before partition\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "\n",
    "    pad_h = (window_size - H % window_size) % window_size\n",
    "    pad_w = (window_size - W % window_size) % window_size\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n",
    "    Hp, Wp = H + pad_h, W + pad_w\n",
    "\n",
    "    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows, (Hp, Wp)\n",
    "\n",
    "\n",
    "def window_unpartition(\n",
    "    windows: torch.Tensor, window_size: int, pad_hw: Tuple[int, int], hw: Tuple[int, int]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Window unpartition into original sequences and removing padding.\n",
    "    Args:\n",
    "        windows (tensor): input tokens with [B * num_windows, window_size, window_size, C].\n",
    "        window_size (int): window size.\n",
    "        pad_hw (Tuple): padded height and width (Hp, Wp).\n",
    "        hw (Tuple): original height and width (H, W) before padding.\n",
    "\n",
    "    Returns:\n",
    "        x: unpartitioned sequences with [B, H, W, C].\n",
    "    \"\"\"\n",
    "    Hp, Wp = pad_hw\n",
    "    H, W = hw\n",
    "    B = windows.shape[0] // (Hp * Wp // window_size // window_size)\n",
    "    x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)\n",
    "\n",
    "    if Hp > H or Wp > W:\n",
    "        x = x[:, :H, :W, :].contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get relative positional embeddings according to the relative positions of\n",
    "        query and key sizes.\n",
    "    Args:\n",
    "        q_size (int): size of query q.\n",
    "        k_size (int): size of key k.\n",
    "        rel_pos (Tensor): relative position embeddings (L, C).\n",
    "\n",
    "    Returns:\n",
    "        Extracted positional embeddings according to relative positions.\n",
    "    \"\"\"\n",
    "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
    "    # Interpolate rel pos if needed.\n",
    "    if rel_pos.shape[0] != max_rel_dist:\n",
    "        # Interpolate rel pos.\n",
    "        rel_pos_resized = F.interpolate(\n",
    "            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n",
    "            size=max_rel_dist,\n",
    "            mode=\"linear\",\n",
    "        )\n",
    "        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n",
    "    else:\n",
    "        rel_pos_resized = rel_pos\n",
    "\n",
    "    # Scale the coords with short length if shapes for q and k are different.\n",
    "    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
    "    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
    "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
    "\n",
    "    return rel_pos_resized[relative_coords.long()]\n",
    "\n",
    "\n",
    "def add_decomposed_rel_pos(\n",
    "    attn: torch.Tensor,\n",
    "    q: torch.Tensor,\n",
    "    rel_pos_h: torch.Tensor,\n",
    "    rel_pos_w: torch.Tensor,\n",
    "    q_size: Tuple[int, int],\n",
    "    k_size: Tuple[int, int],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n",
    "    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950\n",
    "    Args:\n",
    "        attn (Tensor): attention map.\n",
    "        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n",
    "        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
    "        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
    "        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n",
    "        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n",
    "\n",
    "    Returns:\n",
    "        attn (Tensor): attention map with added relative positional embeddings.\n",
    "    \"\"\"\n",
    "    q_h, q_w = q_size\n",
    "    k_h, k_w = k_size\n",
    "    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n",
    "    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n",
    "\n",
    "    B, _, dim = q.shape\n",
    "    r_q = q.reshape(B, q_h, q_w, dim)\n",
    "    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n",
    "    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n",
    "\n",
    "    attn = (\n",
    "        attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n",
    "    ).view(B, q_h * q_w, k_h * k_w)\n",
    "\n",
    "    return attn\n",
    "\n",
    "def closest_numbers(target):\n",
    "    a = int(target ** 0.5)\n",
    "    b = a + 1\n",
    "    while True:\n",
    "        if a * b == target:\n",
    "            return (a, b)\n",
    "        elif a * b < target:\n",
    "            b += 1\n",
    "        else:\n",
    "            a -= 1\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: Tuple[int, int] = (16, 16),\n",
    "        stride: Tuple[int, int] = (16, 16),\n",
    "        padding: Tuple[int, int] = (0, 0),\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kernel_size (Tuple): kernel size of the projection layer.\n",
    "            stride (Tuple): stride of the projection layer.\n",
    "            padding (Tuple): padding size of the projection layer.\n",
    "            in_chans (int): Number of input image channels.\n",
    "            embed_dim (int): Patch embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj(x)\n",
    "        # B C H W -> B H W C\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qkvAttention(nn.Module):\n",
    "    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = True,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n",
    "            rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
    "                positional parameter size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.q= nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.k= nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.v= nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.use_rel_pos = use_rel_pos\n",
    "        if self.use_rel_pos:\n",
    "            assert (\n",
    "                input_size is not None\n",
    "            ), \"Input size must be provided if using relative positional encoding.\"\n",
    "            # initialize relative positional embeddings\n",
    "            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n",
    "            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v:torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W, _ = q.shape\n",
    "        q = self.q(q).reshape(B, H * W, self.num_heads, -1).permute(0, 2, 1, 3).reshape(B*self.num_heads, H*W, -1)\n",
    "        k = self.k(k).reshape(B, H * W, self.num_heads, -1).permute(0, 2, 1, 3).reshape(B*self.num_heads, H*W, -1)\n",
    "        v = self.v(v).reshape(B, H * W, self.num_heads, -1).permute(0, 2, 1, 3).reshape(B*self.num_heads, H*W, -1)\n",
    "\n",
    "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
    "\n",
    "        if self.use_rel_pos:\n",
    "            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attention=qkvAttention(dim=1024,num_heads=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand((1,64,64,1024))\n",
    "y=torch.rand((25,14,14,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[25, 196, 8, -1]' is invalid for input of size 4194304",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcross_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sambal/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sambal/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 45\u001b[0m, in \u001b[0;36mqkvAttention.forward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m     43\u001b[0m B, H, W, _ \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     44\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq(q)\u001b[38;5;241m.\u001b[39mreshape(B, H \u001b[38;5;241m*\u001b[39m W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H\u001b[38;5;241m*\u001b[39mW, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H\u001b[38;5;241m*\u001b[39mW, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv(v)\u001b[38;5;241m.\u001b[39mreshape(B, H \u001b[38;5;241m*\u001b[39m W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H\u001b[38;5;241m*\u001b[39mW, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[25, 196, 8, -1]' is invalid for input of size 4194304"
     ]
    }
   ],
   "source": [
    "cross_attention(y,x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Space_Adapter = qkvAttention(dim=1280, num_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand((1,64,64,1280))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 1280])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Space_Adapter(x,x,x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=SingleCNNEmbed(patchsize=16,in_chans=3,embed_dim=1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024, 1024])\n",
      "torch.Size([1, 64, 1024, 1024])\n",
      "torch.Size([1, 128, 512, 512])\n",
      "torch.Size([1, 256, 256, 256])\n",
      "torch.Size([1, 512, 128, 128])\n",
      "torch.Size([1, 1280, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 1280])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.rand((1,3,1024,1024))\n",
    "cnn(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SingleConv(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False),\n",
    "            LayerNorm2d(out_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class SingleCNNEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        patchsize: int = 8,\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patch_size (int): kernel size of the tokenization layer.\n",
    "            in_chans (int): Number of input image channels.\n",
    "            embed_dim (int): Patch embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        downtimes = 10\n",
    "        mid_channel = 64\n",
    "        self.inc = SingleConv(in_chans, mid_channel)\n",
    "        self.downs = nn.ModuleList()\n",
    "        conv_list=[2,1,2,1,2,1,2,1,2,1]\n",
    "        for i in range(downtimes):\n",
    "            if i == downtimes-1 or conv_list[i]==2:\n",
    "                down = SingleDown(mid_channel, mid_channel)\n",
    "            else:\n",
    "                down = SingleConv(mid_channel, mid_channel*2)\n",
    "                mid_channel = mid_channel*2\n",
    "            self.downs.append(down)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(x.size())\n",
    "        x = self.inc(x)\n",
    "        print(x.size())\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            print(x.size())\n",
    "            #print(x.size())\n",
    "        # B C H W -> B H W C\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "class SingleDown(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False),\n",
    "            LayerNorm2d(out_channels),\n",
    "            nn.GELU()     #nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand((1,3,1024,1024))\n",
    "cnn_model=SingleCNNEmbed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 1024, 1024])\n",
      "torch.Size([1, 64, 512, 512])\n",
      "torch.Size([1, 128, 512, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 64, 3, 3], expected input[1, 128, 256, 256] to have 64 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mSingleCNNEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m down \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdowns:\n\u001b[0;32m---> 55\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m#print(x.size())\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# B C H W -> B H W C\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 75\u001b[0m, in \u001b[0;36mSingleDown.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 64, 3, 3], expected input[1, 128, 256, 256] to have 64 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "cnn_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 64, 64])\n",
      "torch.Size([1, 256, 64, 64])\n",
      "torch.Size([1, 768, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming your patch embeddings have shape (1, 64, 64, 768)\n",
    "patch_embeddings = torch.randn((1, 64, 64, 768))\n",
    "patch_embeddings = patch_embeddings.permute(0,3,1,2)\n",
    "\n",
    "conv_low_pass = nn.Conv2d(768, 256, kernel_size=(1, 1))(patch_embeddings)\n",
    "# 1x1 Convolution\n",
    "conv1x1 = nn.Conv2d(256, 256, kernel_size=(1, 1))(conv_low_pass)\n",
    "# 3x3 Convolution\n",
    "conv3x3 = nn.Conv2d(256, 256, kernel_size=(3, 3), padding=1)(conv_low_pass)\n",
    "# 5x5 Convolution\n",
    "conv5x5 = nn.Conv2d(256, 256, kernel_size=(5, 5), padding=2)(conv_low_pass)\n",
    "\n",
    "print(conv1x1.size())\n",
    "print(conv3x3.size())\n",
    "# Concatenate the results along the last axis (axis=1 in PyTorch)\n",
    "inception_output = torch.cat([conv1x1, conv3x3, conv5x5], dim=1)\n",
    "\n",
    "# You can add more layers or perform further operations as needed\n",
    "\n",
    "# Print the shape of the output\n",
    "print(inception_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "    \n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "\n",
    "        int_dim=in_channels//3\n",
    "        self.conv_low_pass = nn.Conv2d(in_channels,int_dim, kernel_size=(1, 1))\n",
    "        # 1x1 Convolution\n",
    "        self.conv1x1 = nn.Conv2d(int_dim,int_dim, kernel_size=(1, 1))\n",
    "\n",
    "        # 3x3 Convolution\n",
    "        self.conv3x3 = nn.Conv2d(int_dim,int_dim, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "        # 5x5 Convolution\n",
    "        self.conv5x5 = nn.Conv2d(int_dim,int_dim, kernel_size=(5, 5), padding=2)\n",
    "        self.proj=nn.Linear(in_channels,in_channels)\n",
    "        self.norm=LayerNorm2d(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        conv_low_pass=self.conv_low_pass(x.permute(0,3,1,2))\n",
    "        # Apply each convolutional layer\n",
    "        out1x1 = self.conv1x1(conv_low_pass)\n",
    "        out3x3 = self.conv3x3(conv_low_pass)\n",
    "        out5x5 = self.conv5x5(conv_low_pass)\n",
    "\n",
    "        # Concatenate along the last axis (axis=1 in PyTorch)\n",
    "        concat_feature=torch.cat([out1x1, out3x3, out5x5], dim=1)\n",
    "        concat_feature=self.norm(concat_feature).permute(0,2,3,1)\n",
    "        out_feature=self.proj(concat_feature)+x\n",
    "        \n",
    "        return out_feature\n",
    "\n",
    "# Assuming your patch embeddings have shape (1, 64, 64, 768)\n",
    "patch_embeddings = torch.randn((1,64,64,768))\n",
    "\n",
    "# Instantiate the InceptionModule with input channels 768 and output channels 256\n",
    "inception_module = InceptionModule(in_channels=768, out_channels=256)\n",
    "\n",
    "# Forward pass through the module\n",
    "inception_output = inception_module(patch_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLoRAAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"LoRA Attention block with relative position embeddings.\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     10\u001b[0m         dim: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m         lora_rank: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m  \u001b[38;5;66;03m# Specify the desired rank for low-rank approximation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m, in \u001b[0;36mLoRAAttention\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLoRAAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"LoRA Attention block with relative position embeddings.\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     10\u001b[0m         dim: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     11\u001b[0m         num_heads: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     12\u001b[0m         qkv_bias: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m         use_rel_pos: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m         rel_pos_zero_init: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m---> 15\u001b[0m         input_size: \u001b[43mOptional\u001b[49m[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m         lora_lambda: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     17\u001b[0m         lora_alpha: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     18\u001b[0m         lora_rank: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m  \u001b[38;5;66;03m# Specify the desired rank for low-rank approximation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m        Args:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m            dim (int): Number of input channels.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m            lora_rank (int): Rank for low-rank approximation.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LoRAAttention(nn.Module):\n",
    "    \"\"\"LoRA Attention block with relative position embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = True,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "        lora_lambda: float = 1.0,\n",
    "        lora_alpha: float = 1.0,\n",
    "        lora_rank: int = 32  # Specify the desired rank for low-rank approximation\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
    "                positional parameter size.\n",
    "            lora_lambda (float): LoRA parameter for global context information.\n",
    "            lora_alpha (float): LoRA parameter for global context information.\n",
    "            lora_rank (int): Rank for low-rank approximation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.use_rel_pos = use_rel_pos\n",
    "        if self.use_rel_pos:\n",
    "            assert (\n",
    "                input_size is not None\n",
    "            ), \"Input size must be provided if using relative positional encoding.\"\n",
    "            # initialize relative positional embeddings\n",
    "            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n",
    "            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n",
    "\n",
    "        # LoRA parameters\n",
    "        self.lora_lambda = lora_lambda\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_rank = lora_rank\n",
    "\n",
    "    def low_rank_approximation(self, tensor):\n",
    "        \"\"\"Low-rank approximation using SVD.\"\"\"\n",
    "        u, s, v = torch.svd(tensor)\n",
    "        rank = min(self.lora_rank, min(tensor.shape[-2], tensor.shape[-1]))\n",
    "        u = u[:, :, :rank]\n",
    "        s = s[:, :rank]\n",
    "        v = v[:, :, :rank]\n",
    "        return torch.matmul(u, torch.matmul(torch.diag_embed(s), v.transpose(-2, -1)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W, _ = x.shape\n",
    "        # qkv with shape (3, B, nHead, H * W, C)\n",
    "        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        # q, k, v with shape (B * nHead, H * W, C)\n",
    "        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\n",
    "\n",
    "        # Standard self-attention\n",
    "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
    "\n",
    "        if self.use_rel_pos:\n",
    "            attn = self.add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        # LoRA modifications\n",
    "        lora_attn = self.lora_alpha * attn + self.lora_lambda * self.low_rank_approximation(attn)\n",
    "\n",
    "        x = (lora_attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def add_decomposed_rel_pos(self, attn, q, rel_pos_h, rel_pos_w, size_h, size_w):\n",
    "        \"\"\"Add decomposed relative positional embeddings.\"\"\"\n",
    "        B, N, _, C = q.shape\n",
    "        q = q.reshape(B, self.num_heads, N, C // self.num_heads).permute(0, 1, 3, 2)\n",
    "        rel_pos = rel_pos_h[:, None, :, None] + rel_pos_w[None, :, None, :]\n",
    "        rel_pos_emb = rel_pos.permute(2, 0, 1).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        attn += (q @ rel_pos_emb).sum(dim=-1)\n",
    "        return attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Parameter(torch.Tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sambal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
